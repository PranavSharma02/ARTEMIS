#!/usr/bin/env python3
"""
RL-MAREA Hybrid System: Reinforcement Learning Enhanced MAREA Ensemble

This module combines the high-performance MAREA ensemble with strategic RL components
to create a hybrid system that maintains performance while gaining RL benefits.

Key Innovations:
1. Hierarchical RL-Supervised Architecture
2. Multi-Agent RL Coordination
3. RL-Enhanced Position Sizing
4. Conservative RL Integration Strategy
5. Performance-Preserving Design

Research Publication Ready: Demonstrates advanced RL-Supervised Learning fusion
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Normal, Categorical
import numpy as np
import pandas as pd
from collections import deque, namedtuple
import random
from typing import List, Dict, Tuple, Optional
import copy

from marea_ensemble_system import MAREAEnsembleSystem
from marea_diverse_architectures import create_diverse_model_configs
from return_optimizer import UltraReturnBoostLoss
from tradingPerformance import PerformanceEstimator
from advanced_rl_algorithms import (
    TD3TradingAgent, SACTradingAgent, TradingRewardFunction,
    RiskAwareRLWrapper, create_risk_aware_agent
)

# RL Experience and Components
Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

class AdvancedReplayBuffer:
    """Advanced replay buffer with prioritized experience replay"""
    def __init__(self, capacity=100000, alpha=0.6, beta=0.4):
        self.capacity = capacity
        self.alpha = alpha  # Prioritization strength
        self.beta = beta    # Importance sampling correction
        self.buffer = []
        self.priorities = deque(maxlen=capacity)
        self.position = 0
        
    def push(self, experience, td_error=None):
        """Add experience with priority"""
        max_priority = max(self.priorities) if self.priorities else 1.0
        priority = max_priority if td_error is None else abs(td_error) + 1e-6
        
        if len(self.buffer) < self.capacity:
            self.buffer.append(experience)
            self.priorities.append(priority)
        else:
            self.buffer[self.position] = experience
            self.priorities[self.position] = priority
        
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size):
        """Sample batch with prioritized replay"""
        if len(self.buffer) < batch_size:
            return None
            
        priorities = np.array(self.priorities)
        probabilities = priorities ** self.alpha
        probabilities /= probabilities.sum()
        
        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)
        experiences = [self.buffer[idx] for idx in indices]
        
        # Importance sampling weights
        weights = (len(self.buffer) * probabilities[indices]) ** (-self.beta)
        weights /= weights.max()
        
        return experiences, indices, weights

class MAREACoordinatorAgent(nn.Module):
    """
    Hierarchical RL Agent that coordinates the MAREA ensemble models
    This preserves the supervised learning predictions while optimizing coordination
    """
    def __init__(self, n_models=5, state_dim=128, hidden_dim=256):
        super().__init__()
        self.n_models = n_models
        self.state_dim = state_dim
        
        # State processing
        self.state_encoder = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # Model weight coordination (outputs softmax weights for ensemble)
        self.weight_head = nn.Sequential(
            nn.Linear(hidden_dim//2, 64),
            nn.ReLU(),
            nn.Linear(64, n_models)
        )
        
        # Value function for RL training
        self.value_head = nn.Sequential(
            nn.Linear(hidden_dim//2, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        # Confidence estimation
        self.confidence_head = nn.Sequential(
            nn.Linear(hidden_dim//2, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        
    def forward(self, state, model_predictions):
        """
        Coordinate MAREA models without disrupting their predictions
        """
        # Encode market state
        encoded_state = self.state_encoder(state)
        
        # Generate coordination weights
        raw_weights = self.weight_head(encoded_state)
        coord_weights = F.softmax(raw_weights, dim=-1)
        
        # Estimate value and confidence
        value = self.value_head(encoded_state)
        confidence = self.confidence_head(encoded_state)
        
        # Coordinated prediction (preserves original model capabilities)
        coordinated_pred = torch.sum(coord_weights * model_predictions, dim=-1, keepdim=True)
        
        return {
            'weights': coord_weights,
            'coordinated_prediction': coordinated_pred,
            'value': value,
            'confidence': confidence,
            'encoded_state': encoded_state
        }

class MAREAPositionSizerAgent(nn.Module):
    """
    RL Agent for dynamic position sizing optimization
    Focuses on risk-adjusted position sizing without disrupting signal generation
    """
    def __init__(self, state_dim=64, hidden_dim=128):
        super().__init__()
        
        self.state_processor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU()
        )
        
        # Position sizing policy (continuous action space)
        self.position_mean = nn.Sequential(
            nn.Linear(hidden_dim//2, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Tanh()  # Output between -1 and 1
        )
        
        self.position_std = nn.Sequential(
            nn.Linear(hidden_dim//2, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Softplus()  # Ensure positive std
        )
        
        self.value = nn.Sequential(
            nn.Linear(hidden_dim//2, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
        
    def forward(self, state):
        processed = self.state_processor(state)
        
        mean = self.position_mean(processed)
        std = self.position_std(processed) + 1e-6  # Numerical stability
        value = self.value(processed)
        
        return mean, std, value

class MAREARegimeAgent(nn.Module):
    """
    RL Agent for market regime detection and adaptation
    Enhances the existing regime detection with RL-based adaptation
    """
    def __init__(self, state_dim=96, n_regimes=5, hidden_dim=128):
        super().__init__()
        self.n_regimes = n_regimes
        
        self.regime_encoder = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU()
        )
        
        # Regime classification with confidence
        self.regime_classifier = nn.Sequential(
            nn.Linear(hidden_dim//2, 64),
            nn.ReLU(),
            nn.Linear(64, n_regimes)
        )
        
        self.regime_confidence = nn.Sequential(
            nn.Linear(hidden_dim//2, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        
        self.value = nn.Sequential(
            nn.Linear(hidden_dim//2, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
        
    def forward(self, state):
        encoded = self.regime_encoder(state)
        
        regime_logits = self.regime_classifier(encoded)
        regime_probs = F.softmax(regime_logits, dim=-1)
        confidence = self.regime_confidence(encoded)
        value = self.value(encoded)
        
        return regime_probs, confidence, value

class HybridRLMAREASystem(MAREAEnsembleSystem):
    """
    Hybrid RL-MAREA System that combines the best of both paradigms
    
    Key Design Principles:
    1. Preserve MAREA supervised learning performance
    2. Add RL for coordination and optimization
    3. Conservative integration to maintain Sharpe ratios
    4. Multi-agent architecture for specialized RL tasks
    5. Performance monitoring and adaptive learning
    """
    
    def __init__(self, sequence_length=60, initial_balance=100000, device=None,
                 return_boost_factor=1.25, ultra_aggressive_mode=True,
                 rl_learning_rate=1e-4, rl_update_frequency=10, rl_agent_type='td3'):
        super().__init__(sequence_length, initial_balance, device, 
                        return_boost_factor, ultra_aggressive_mode)
        
        self.framework_name = "Hybrid-RL-MAREA"
        self.version = "2.0"
        self.rl_learning_rate = rl_learning_rate
        self.rl_update_frequency = rl_update_frequency
        self.rl_agent_type = rl_agent_type.lower()
        
        # RL Components
        self.coordinator_agent = None
        self.position_agent = None
        self.regime_agent = None
        self.advanced_rl_agent = None  # NEW: Advanced RL agent
        
        # Experience replay and training
        self.replay_buffer = AdvancedReplayBuffer(capacity=50000)
        self.rl_training_step = 0
        self.performance_history = deque(maxlen=1000)
        
        # Performance monitoring for adaptive RL
        self.supervised_baseline_performance = None
        self.hybrid_performance_threshold = 0.98  # Maintain 98% of supervised performance
        
        # Enhanced trading reward function optimized for MAREA outperformance
        self.reward_function = TradingRewardFunction(
            risk_aversion=0.15,
            transaction_cost=0.0003,
            sharpe_weight=0.4,
            return_weight=0.5,
            drawdown_weight=0.1,
            baseline_outperformance_weight=0.3
        )
        
        # Enhanced GPU setup and optimization
        self._setup_gpu_optimization()
        
        print(f"üß† {self.framework_name} v{self.version} initialized")
        print(f"   Hybrid RL-Supervised Learning Architecture")
        print(f"   Advanced {rl_agent_type.upper()} RL agent integration")
        print(f"   Conservative RL integration for performance preservation")
        print(f"   üöÄ Device: {self.device}")
        if self.device.type == 'cuda':
            print(f"   üíé GPU: {torch.cuda.get_device_name(0)}")
            print(f"   üîã VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
            print(f"   ‚ö° CUDA Capability: {torch.cuda.get_device_capability(0)}")
        else:
            print(f"   ‚ö†Ô∏è  Using CPU - GPU acceleration not available")
    
    def _setup_gpu_optimization(self):
        """Setup GPU optimization and memory management"""
        if self.device.type == 'cuda':
            # Enable optimized CUDA operations
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # Set memory allocation strategy
            torch.cuda.empty_cache()
            
            # Enable automatic mixed precision if supported
            if hasattr(torch.cuda.amp, 'autocast'):
                self.use_amp = True
                self.amp_scaler = torch.cuda.amp.GradScaler()  # Renamed to avoid conflict
                print(f"   ‚ö° Automatic Mixed Precision enabled")
            else:
                self.use_amp = False
                
            # Set optimal number of threads for GPU
            torch.set_num_threads(min(8, torch.get_num_threads()))
            
            print(f"   üîß GPU optimizations applied")
        else:
            self.use_amp = False
            # Optimize for CPU
            torch.set_num_threads(min(8, torch.get_num_threads()))
    
    def _ensure_tensor_device(self, tensor):
        """Ensure tensor is on the correct device"""
        if isinstance(tensor, torch.Tensor):
            return tensor.to(self.device, non_blocking=True)
        elif isinstance(tensor, (list, tuple)):
            return [self._ensure_tensor_device(t) for t in tensor]
        elif isinstance(tensor, dict):
            return {k: self._ensure_tensor_device(v) for k, v in tensor.items()}
        else:
            return tensor
        
    def initialize_rl_agents(self):
        """Initialize RL agents after MAREA models are trained"""
        if len(self.models) == 0:
            print("‚ùå MAREA models must be trained first!")
            return
            
        print(f"ü§ñ Initializing RL Agents for Hybrid System...")
        
        # Calculate state dimensions based on available features
        state_dim = min(128, self.X.shape[2])  # Market state dimension
        
        # Initialize agents
        self.coordinator_agent = MAREACoordinatorAgent(
            n_models=len(self.models),
            state_dim=state_dim,
            hidden_dim=256
        ).to(self.device)
        
        self.position_agent = MAREAPositionSizerAgent(
            state_dim=min(64, self.X.shape[2]),
            hidden_dim=128
        ).to(self.device)
        
        self.regime_agent = MAREARegimeAgent(
            state_dim=min(96, self.X.shape[2]),
            n_regimes=5,
            hidden_dim=128
        ).to(self.device)
        
        # Optimizers for RL agents
        self.coordinator_optimizer = optim.Adam(
            self.coordinator_agent.parameters(), lr=self.rl_learning_rate
        )
        self.position_optimizer = optim.Adam(
            self.position_agent.parameters(), lr=self.rl_learning_rate
        )
        self.regime_optimizer = optim.Adam(
            self.regime_agent.parameters(), lr=self.rl_learning_rate
        )
        
        # Initialize advanced RL agent for core trading decisions
        state_dim = min(128, self.X.shape[2])
        action_dim = 1  # Single continuous trading action
        
        self.advanced_rl_agent = create_risk_aware_agent(
            base_agent_type=self.rl_agent_type,
            state_dim=state_dim,
            action_dim=action_dim,
            risk_management=True,
            lr=self.rl_learning_rate,
            max_position_size=1.0,
            stop_loss_threshold=-0.05,
            max_daily_loss=-0.02
        )
        
        print(f"   ‚úÖ Coordinator Agent: {len(self.models)} model coordination")
        print(f"   ‚úÖ Position Sizer Agent: Dynamic risk-adjusted sizing")
        print(f"   ‚úÖ Regime Agent: Enhanced market regime detection")
        print(f"   ‚úÖ Advanced RL Agent: {self.rl_agent_type.upper()} with risk management")
        
    def train_hybrid_system(self, n_supervised_models=5, supervised_epochs=250,
                          rl_episodes=500, evaluation_frequency=50):
        """
        Train the complete hybrid system with conservative RL integration
        """
        print(f"üî• HYBRID RL-MAREA TRAINING PIPELINE")
        print(f"   Phase 1: Train MAREA supervised ensemble")
        print(f"   Phase 2: Initialize and train RL agents")
        print(f"   Phase 3: Joint optimization with performance monitoring")
        
        # Phase 1: Train supervised MAREA models (baseline performance)
        print(f"\nüìö Phase 1: Training MAREA Supervised Ensemble...")
        self.train_marea_ultra_aggressive_ensemble(n_models=n_supervised_models, epochs=supervised_epochs)
        
        # Establish baseline performance
        baseline_signals = self.generate_marea_ultra_aggressive_signals()
        baseline_results = self.backtest_signals(baseline_signals)
        self.supervised_baseline_performance = {
            'annual_return': baseline_results['annual_return'],
            'sharpe_ratio': baseline_results['sharpe_ratio'],
            'max_drawdown': baseline_results['max_drawdown']
        }
        
        print(f"   üìä Baseline Performance Established:")
        print(f"   Annual Return: {baseline_results['annual_return']:.2%}")
        print(f"   Sharpe Ratio: {baseline_results['sharpe_ratio']:.3f}")
        print(f"   Max Drawdown: {baseline_results['max_drawdown']:.2%}")
        
        # Phase 2: Initialize RL agents
        print(f"\nü§ñ Phase 2: Initializing RL Agents...")
        self.initialize_rl_agents()
        
        # Phase 3: Conservative RL training with performance monitoring
        print(f"\nüéØ Phase 3: Conservative RL Training ({rl_episodes} episodes)...")
        
        best_hybrid_performance = float('-inf')
        consecutive_poor_performance = 0
        performance_threshold_violations = 0
        
        for episode in range(rl_episodes):
            # Generate hybrid signals
            hybrid_signals = self.generate_hybrid_rl_signals()
            hybrid_results = self.backtest_signals(hybrid_signals)
            
            # Enhanced performance monitoring for learning optimization
            current_performance = hybrid_results['sharpe_ratio']
            baseline_sharpe = self.supervised_baseline_performance['sharpe_ratio']
            
            # Track performance ratio for learning insights
            performance_ratio = current_performance / baseline_sharpe if baseline_sharpe != 0 else 0
            
            # Only count as violation if significantly underperforming
            if performance_ratio < 0.85:  # More lenient threshold for learning
                performance_threshold_violations += 1
                consecutive_poor_performance += 1
            else:
                consecutive_poor_performance = 0
                
            # Record performance
            self.performance_history.append({
                'episode': episode,
                'annual_return': hybrid_results['annual_return'],
                'sharpe_ratio': current_performance,
                'max_drawdown': hybrid_results['max_drawdown'],
                'performance_ratio': performance_ratio
            })
            
            # RL training step
            if episode % self.rl_update_frequency == 0 and episode > 0:
                self._update_rl_agents(hybrid_results)
            
            # Adaptive learning rate adjustment
            if consecutive_poor_performance > 10:
                self._reduce_rl_learning_rates()
                consecutive_poor_performance = 0
                print(f"   ‚ö†Ô∏è  Performance decline detected, reducing RL learning rates")
            
            # Track best performance
            if current_performance > best_hybrid_performance:
                best_hybrid_performance = current_performance
                self._save_best_rl_checkpoint()
            
            # Progress reporting
            if episode % evaluation_frequency == 0:
                print(f"   Episode {episode:3d}: Sharpe={current_performance:.3f}, "
                      f"Ratio={performance_ratio:.3f}, Violations={performance_threshold_violations}")
                
            # Adaptive learning instead of early stopping
            if performance_threshold_violations > rl_episodes * 0.6:  # More lenient
                print(f"   ‚ö†Ô∏è  High violation rate, reducing learning rate for stability")
                self._reduce_rl_learning_rates(factor=0.7)
                performance_threshold_violations = max(0, performance_threshold_violations - 5)  # Reset partially
        
        print(f"\nüèÜ HYBRID RL-MAREA TRAINING COMPLETE!")
        print(f"   Best Hybrid Sharpe: {best_hybrid_performance:.3f}")
        print(f"   Performance Violations: {performance_threshold_violations}/{rl_episodes}")
        
        return self.models
    
    def generate_hybrid_rl_signals(self, start_idx=None, end_idx=None):
        """
        Generate trading signals using the hybrid RL-MAREA system
        """
        if not all([self.coordinator_agent, self.position_agent, self.regime_agent]):
            print("‚ö†Ô∏è  RL agents not initialized, falling back to supervised signals")
            return self.generate_marea_ultra_aggressive_signals(start_idx, end_idx)
        
        print(f"üéØ Generating Hybrid RL-MAREA Signals...")
        
        # Get supervised predictions from MAREA models
        X_sequences = self.X[start_idx:end_idx] if start_idx is not None else self.X
        
        base_predictions = []
        model_confidence_scores = []
        
        # Get predictions from all MAREA models (supervised component) with GPU optimization
        for i, model in enumerate(self.models):
            model.eval()
            
            # Ensure model is on correct device
            model = model.to(self.device)
            
            # Ensure input tensor is on correct device
            X_sequences_gpu = self._ensure_tensor_device(X_sequences)
            
            # Use automatic mixed precision if available for better GPU utilization
            if self.use_amp and self.device.type == 'cuda':
                with torch.cuda.amp.autocast():
                    with torch.no_grad():
                        preds = model(X_sequences_gpu).cpu().numpy().flatten()
            else:
                with torch.no_grad():
                    preds = model(X_sequences_gpu).cpu().numpy().flatten()
                    
            base_predictions.append(preds)
            
            # Simple confidence estimation based on prediction consistency
            confidence = 1.0 - np.std(preds) if len(preds) > 1 else 1.0
            model_confidence_scores.append(confidence)
            
            # Clear GPU cache periodically to prevent memory issues
            if self.device.type == 'cuda' and i % 2 == 0:
                torch.cuda.empty_cache()
        
        base_predictions = np.array(base_predictions).T  # Shape: (timesteps, n_models)
        final_signals = np.zeros(len(base_predictions))
        
        # Advanced Multi-Agent RL Enhancement with Sophisticated Integration
        previous_portfolio_value = 1.0
        daily_pnl = 0.0
        
        for i in range(len(base_predictions)):
            current_state = X_sequences[i:i+1, -1, :]  # Last timestep features
            
            # Prepare state tensors for different agents - ensure GPU placement
            coord_state = self._ensure_tensor_device(current_state[:, :min(128, current_state.shape[1])])
            position_state = self._ensure_tensor_device(current_state[:, :min(64, current_state.shape[1])])
            regime_state = self._ensure_tensor_device(current_state[:, :min(96, current_state.shape[1])])
            rl_state = current_state[:, :min(128, current_state.shape[1])].cpu().numpy().flatten()
            
            # RL Coordinator Agent with GPU optimization
            model_preds_tensor = self._ensure_tensor_device(torch.FloatTensor([base_predictions[i]]))
            
            if self.use_amp and self.device.type == 'cuda':
                with torch.cuda.amp.autocast():
                    coord_output = self.coordinator_agent(coord_state, model_preds_tensor)
            else:
                coord_output = self.coordinator_agent(coord_state, model_preds_tensor)
            
            # RL Position Sizer Agent with GPU optimization
            if self.use_amp and self.device.type == 'cuda':
                with torch.cuda.amp.autocast():
                    pos_mean, pos_std, pos_value = self.position_agent(position_state)
            else:
                pos_mean, pos_std, pos_value = self.position_agent(position_state)
                
            position_dist = Normal(pos_mean, pos_std)
            position_multiplier = torch.clamp(position_dist.sample(), -1.0, 1.0)
            
            # RL Regime Agent with GPU optimization
            if self.use_amp and self.device.type == 'cuda':
                with torch.cuda.amp.autocast():
                    regime_probs, regime_conf, regime_value = self.regime_agent(regime_state)
            else:
                regime_probs, regime_conf, regime_value = self.regime_agent(regime_state)
                
            dominant_regime = torch.argmax(regime_probs, dim=-1).cpu().item()
            
            # Advanced RL Agent (NEW: Core trading decision)
            rl_action = self.advanced_rl_agent.select_action(rl_state, add_noise=True)
            
            # Sophisticated Multi-Agent RL Signal Enhancement
            supervised_signal = np.average(base_predictions[i])  # MAREA ensemble baseline
            rl_coordinated_signal = coord_output['coordinated_prediction'].cpu().item()
            advanced_rl_signal = rl_action[0] if hasattr(rl_action, '__len__') else rl_action
            
            # Advanced confidence weighting with volatility adjustment
            coord_confidence = coord_output['confidence'].cpu().item()
            regime_confidence = regime_conf.cpu().item()
            
            # Market state analysis for intelligent blending
            market_volatility = np.std(base_predictions[max(0, i-10):i+1]) if i > 10 else 0.1
            volatility_factor = min(1.5, max(0.5, 1.0 / (1.0 + market_volatility)))
            
            # Stage 1: Signal amplification - RL enhances rather than dampens
            signal_strength = abs(supervised_signal)
            
            # RL coordination for signal timing optimization
            coord_amplification = 1.0 + coord_confidence * 0.3  # Amplify when confident
            rl_timing_factor = np.tanh(rl_coordinated_signal * 1.5)  # RL timing signal
            coordinated_signal = supervised_signal * coord_amplification * (1.0 + rl_timing_factor * 0.2)
            
            # Stage 2: Intelligent signal enhancement based on RL insights
            # RL learns WHEN to be more aggressive, not less
            regime_amplifier = 1.0 + regime_confidence * 0.4
            rl_enhancement_factor = np.tanh(advanced_rl_signal * 2.0)  # RL enhancement
            
            # Apply RL enhancement multiplicatively (amplifies rather than dampens)
            enhanced_signal = coordinated_signal * regime_amplifier * (1.0 + rl_enhancement_factor * 0.25)
            
            # Stage 2b: Aggressive amplification for strong MAREA signals
            if abs(supervised_signal) > 0.6:  # Strong MAREA signal
                confidence_boost = (signal_strength - 0.6) * 1.5  # More aggressive boost
                enhanced_signal *= (1.0 + confidence_boost)
            
            # Stage 3: Aggressive position sizing optimization
            position_mult = position_multiplier.cpu().item()
            # Amplify position sizing when both MAREA and RL are confident
            combined_confidence = min(1.5, signal_strength + coord_confidence + regime_confidence)
            position_amplification = 1.0 + 0.3 * (position_mult * combined_confidence)
            position_adjusted_signal = enhanced_signal * position_amplification
            
            # Stage 4: Regime-optimized aggressive positioning  
            regime_multiplier = 1.0
            
            if dominant_regime == 0:  # Bull market - maximum momentum capture
                regime_multiplier = 1.15 + 0.10 * regime_confidence
                if supervised_signal > 0:  # Extra aggressive for bull signals
                    regime_multiplier += 0.08
            elif dominant_regime == 1:  # Bear market - aggressive short positioning
                regime_multiplier = 1.05  # Still amplify (smart bear positioning)
                if supervised_signal < 0:  # Amplify bear signals in bear market
                    regime_multiplier += 0.08 + 0.05 * regime_confidence
            elif dominant_regime == 2:  # Sideways - tactical trading
                regime_multiplier = 1.05 + abs(supervised_signal) * 0.15  # More with stronger signals
            elif dominant_regime == 3:  # High volatility - volatility trading
                regime_multiplier = 1.08 + abs(supervised_signal) * 0.12  # Capitalize on volatility
            
            # Stage 5: Final aggressive optimization
            momentum_amplifier = 1.0 + abs(np.tanh(supervised_signal * 1.8)) * 0.12
            final_signal = position_adjusted_signal * regime_multiplier * momentum_amplifier
            
            # Apply safety bounds with dynamic risk management
            max_position = 1.0 if regime_confidence > 0.7 else 0.8
            final_signals[i] = np.clip(final_signal, -max_position, max_position)
            
            # Update RL agent's learning system
            if i > 0:
                # Calculate portfolio return for RL learning
                current_return = final_signals[i] * 0.01  # Simplified return calculation
                daily_pnl += current_return
                portfolio_value = previous_portfolio_value * (1 + current_return)
                position_pnl = (portfolio_value - previous_portfolio_value) / previous_portfolio_value
                
                # Update RL risk management
                if hasattr(self.advanced_rl_agent, 'update_pnl'):
                    self.advanced_rl_agent.update_pnl(daily_pnl, position_pnl)
                
                previous_portfolio_value = portfolio_value
        
        print(f"   üöÄ Generated {len(final_signals)} hybrid RL-MAREA signals")
        print(f"   üìà Signal range: [{final_signals.min():.3f}, {final_signals.max():.3f}]")
        print(f"   ü§ñ Advanced Multi-Agent RL Enhancement with Market Intelligence")
        
        return final_signals
    
    def _update_rl_agents(self, results):
        """Enhanced RL agent updates with advanced reward calculation"""
        if len(self.performance_history) < 2:
            return
            
        # Calculate advanced reward using trading-specific reward function
        current_perf = self.performance_history[-1]
        previous_perf = self.performance_history[-2]
        
        # Portfolio return for this episode
        portfolio_return = current_perf['annual_return'] - previous_perf['annual_return']
        
        # Use advanced reward function
        reward, reward_components = self.reward_function.calculate_reward(
            portfolio_return=portfolio_return,
            action=0.1,  # Simplified action representation
            market_return=None
        )
        
        # Update advanced RL agent if it supports training
        if hasattr(self.advanced_rl_agent, 'train') and hasattr(self.advanced_rl_agent, 'base_agent'):
            # For risk-aware wrapper, get the base agent
            base_agent = getattr(self.advanced_rl_agent, 'base_agent', self.advanced_rl_agent)
            
            if hasattr(base_agent, 'update_performance'):
                base_agent.update_performance(reward)
                
            # Train with replay buffer if available
            if len(self.replay_buffer.buffer) > 100:
                training_losses = base_agent.train(self.replay_buffer, batch_size=64)
                
                if isinstance(training_losses, dict):
                    print(f"   ü§ñ RL Training - Critic Loss: {training_losses.get('critic_loss', 0):.6f}, "
                          f"Actor Loss: {training_losses.get('actor_loss', 0):.6f}")
        
        # Enhanced coordinator agent updates
        if abs(reward) > 1e-6:
            # Multi-component reward for coordinator
            sharpe_improvement = current_perf['sharpe_ratio'] - previous_perf['sharpe_ratio']
            
            # Conservative coordinator update
            coord_reward = (
                0.4 * reward +                    # Advanced reward
                0.3 * sharpe_improvement +        # Sharpe improvement
                0.3 * portfolio_return            # Portfolio return
            )
            
            coord_loss = -coord_reward * 0.005  # Very conservative updates
            self.coordinator_optimizer.zero_grad()
            coord_loss_tensor = torch.tensor(coord_loss, requires_grad=True, device=self.device)
            coord_loss_tensor.backward()
            torch.nn.utils.clip_grad_norm_(self.coordinator_agent.parameters(), 0.5)
            self.coordinator_optimizer.step()
            
            # Similar updates for position and regime agents
            if abs(sharpe_improvement) > 0.01:  # Only update if significant improvement
                # Position agent update
                pos_loss = -sharpe_improvement * 0.003
                self.position_optimizer.zero_grad()
                pos_loss_tensor = torch.tensor(pos_loss, requires_grad=True, device=self.device)
                pos_loss_tensor.backward()
                torch.nn.utils.clip_grad_norm_(self.position_agent.parameters(), 0.5)
                self.position_optimizer.step()
                
                # Regime agent update  
                regime_loss = -reward * 0.002
                self.regime_optimizer.zero_grad()
                regime_loss_tensor = torch.tensor(regime_loss, requires_grad=True, device=self.device)
                regime_loss_tensor.backward()
                torch.nn.utils.clip_grad_norm_(self.regime_agent.parameters(), 0.5)
                self.regime_optimizer.step()
        
        self.rl_training_step += 1
        
        # Log detailed reward components for analysis
        if self.rl_training_step % 20 == 0:
            print(f"   üìä Reward Components - Total: {reward:.6f}")
            print(f"      Return: {reward_components['return_component']:.6f}")
            print(f"      Transaction: {reward_components['transaction_penalty']:.6f}")
            print(f"      Risk-Adjusted: {reward_components['risk_adjusted_component']:.6f}")
    
    def _reduce_rl_learning_rates(self, factor=0.8):
        """Reduce RL learning rates when performance degrades"""
        for optimizer in [self.coordinator_optimizer, self.position_optimizer, self.regime_optimizer]:
            for param_group in optimizer.param_groups:
                param_group['lr'] *= factor
    
    def _save_best_rl_checkpoint(self):
        """Save best performing RL model state"""
        self.best_rl_state = {
            'coordinator': copy.deepcopy(self.coordinator_agent.state_dict()),
            'position': copy.deepcopy(self.position_agent.state_dict()),
            'regime': copy.deepcopy(self.regime_agent.state_dict())
        }
    
    def _switch_to_conservative_mode(self):
        """Switch to more conservative RL behavior"""
        print("   üõ°Ô∏è  Switching to conservative RL mode...")
        # Reduce RL influence and increase supervised component weight
        self.hybrid_performance_threshold = 0.98  # More strict threshold
        self._reduce_rl_learning_rates(factor=0.5)
    
    def get_performance_comparison(self):
        """Get detailed performance comparison between supervised and hybrid"""
        if not self.performance_history:
            return None
            
        # Calculate average hybrid performance from recent episodes
        performance_list = list(self.performance_history)
        recent_performance = performance_list[-10:] if len(performance_list) >= 10 else performance_list
        
        avg_hybrid_sharpe = np.mean([p['sharpe_ratio'] for p in recent_performance])
        avg_hybrid_return = np.mean([p['annual_return'] for p in recent_performance])
        avg_hybrid_drawdown = np.mean([p['max_drawdown'] for p in recent_performance])
        
        return {
            'supervised_baseline': self.supervised_baseline_performance,
            'hybrid_average': {
                'annual_return': avg_hybrid_return,
                'sharpe_ratio': avg_hybrid_sharpe,
                'max_drawdown': avg_hybrid_drawdown
            },
            'performance_improvement': {
                'sharpe_ratio': avg_hybrid_sharpe - self.supervised_baseline_performance['sharpe_ratio'],
                'annual_return': avg_hybrid_return - self.supervised_baseline_performance['annual_return']
            }
        }
    
    def backtest_signals(self, signals):
        """Backtest trading signals with comprehensive performance metrics"""
        if len(signals) == 0:
            print("No signals to backtest")
            return None
        
        # Align signals with price data
        price_data = self.features_df['Close'].iloc[self.sequence_length:self.sequence_length + len(signals)]
        returns_data = self.features_df['Returns'].iloc[self.sequence_length:self.sequence_length + len(signals)]
        
        # Get date range for proper annual return calculation
        start_date = self.features_df.index[self.sequence_length]
        end_date = self.features_df.index[self.sequence_length + len(signals) - 1]
        trading_days = len(signals)
        years = trading_days / 252.0  # 252 trading days per year
        
        # Trading simulation with GPU optimization if available
        if self.device.type == 'cuda':
            # Move to GPU for faster computation
            signals_tensor = torch.FloatTensor(signals).to(self.device)
            returns_tensor = torch.FloatTensor(returns_data.values).to(self.device)
            
            # Calculate portfolio returns on GPU
            positions = torch.tanh(signals_tensor)  # Convert to position sizes [-1, 1]
            portfolio_returns_tensor = positions * returns_tensor
            portfolio_returns = portfolio_returns_tensor.cpu().numpy()
        else:
            # CPU computation
            positions = np.tanh(signals)  # Convert to position sizes [-1, 1]
            portfolio_returns = positions * returns_data.values
        
        # Calculate cumulative returns (compound growth)
        cumulative_returns = (1 + portfolio_returns).cumprod()
        total_return = cumulative_returns[-1] - 1
        
        # Calculate annualized return
        if years > 0 and total_return > -1:
            annual_return = ((1 + total_return) ** (1 / years)) - 1
        else:
            annual_return = 0
        
        # Calculate Sharpe ratio (annualized)
        excess_returns = portfolio_returns - 0.02/252  # Assume 2% risk-free rate
        if np.std(portfolio_returns) > 0:
            sharpe_ratio = np.mean(excess_returns) / np.std(portfolio_returns) * np.sqrt(252)
        else:
            sharpe_ratio = 0
        
        # Calculate max drawdown
        rolling_max = pd.Series(cumulative_returns).expanding().max()
        drawdowns = (pd.Series(cumulative_returns) / rolling_max) - 1
        max_drawdown = drawdowns.min()
        
        # Win rate
        winning_trades = np.sum(portfolio_returns > 0)
        total_trades = len(portfolio_returns)
        win_rate = winning_trades / total_trades if total_trades > 0 else 0
        
        # Calculate buy & hold performance for comparison
        buy_hold_total_return = (price_data.iloc[-1] / price_data.iloc[0]) - 1
        if years > 0 and buy_hold_total_return > -1:
            buy_hold_annual_return = ((1 + buy_hold_total_return) ** (1 / years)) - 1
        else:
            buy_hold_annual_return = 0
        
        results = {
            'signals': signals,
            'positions': positions if isinstance(positions, np.ndarray) else positions.cpu().numpy(),
            'portfolio_returns': portfolio_returns,
            'cumulative_returns': cumulative_returns,
            'total_return': total_return,
            'annual_return': annual_return,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'win_rate': win_rate,
            'total_trades': total_trades,
            'price_data': price_data,
            'returns_data': returns_data,
            'trading_days': trading_days,
            'years': years,
            'start_date': start_date,
            'end_date': end_date,
            'buy_hold_total_return': buy_hold_total_return,
            'buy_hold_annual_return': buy_hold_annual_return
        }
        
        return results

def main():
    """Demonstration of the Hybrid RL-MAREA System"""
    print("üöÄ Hybrid RL-MAREA System Demonstration")
    
    # Initialize hybrid system
    hybrid_system = HybridRLMAREASystem(
        sequence_length=60,
        initial_balance=100000,
        return_boost_factor=1.25,
        ultra_aggressive_mode=True,
        rl_learning_rate=1e-4
    )
    
    # Load data and prepare
    hybrid_system.load_and_prepare_data(stock_symbol="AAPL")
    hybrid_system.create_enhanced_technical_indicators()
    hybrid_system.prepare_sequences()
    
    # Train hybrid system
    hybrid_system.train_hybrid_system(
        n_supervised_models=5,
        supervised_epochs=200,  # Reduced for demo
        rl_episodes=300,        # Conservative RL training
        evaluation_frequency=30
    )
    
    # Generate and test hybrid signals
    hybrid_signals = hybrid_system.generate_hybrid_rl_signals()
    results = hybrid_system.backtest_signals(hybrid_signals)
    
    # Performance comparison
    comparison = hybrid_system.get_performance_comparison()
    
    print(f"\n{'='*60}")
    print(f"üèÜ HYBRID RL-MAREA PERFORMANCE COMPARISON")
    print(f"{'='*60}")
    
    if comparison:
        print(f"üìä Supervised Baseline:")
        print(f"   Annual Return: {comparison['supervised_baseline']['annual_return']:.2%}")
        print(f"   Sharpe Ratio:  {comparison['supervised_baseline']['sharpe_ratio']:.3f}")
        print(f"   Max Drawdown:  {comparison['supervised_baseline']['max_drawdown']:.2%}")
        
        print(f"\nü§ñ Hybrid RL-Enhanced:")
        print(f"   Annual Return: {comparison['hybrid_average']['annual_return']:.2%}")
        print(f"   Sharpe Ratio:  {comparison['hybrid_average']['sharpe_ratio']:.3f}")
        print(f"   Max Drawdown:  {comparison['hybrid_average']['max_drawdown']:.2%}")
        
        print(f"\nüìà Performance Improvement:")
        print(f"   Sharpe Improvement: {comparison['performance_improvement']['sharpe_ratio']:+.3f}")
        print(f"   Return Improvement: {comparison['performance_improvement']['annual_return']:+.2%}")

if __name__ == "__main__":
    main() 